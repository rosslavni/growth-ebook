数据分析
---

数据分析是一个很有意思的过程，我们可以简单地将这个过程分成四个步骤：

 - 识别需求
 - 收集数据
 - 分析数据
 - 展示数据
 - 改进

值得注意的是：在分析数据的过程中，需要不同的人员来参与，需要跨域多个领域的知识点——分析、设计、开发、商业和研究等领域。因此，在这样的领域里，回归敏捷也是一种不错的选择（源于：《敏捷数据科学》）：

 - 通才高于专长
 - 小团队高于大团队
 - 使用高阶工具和平台：云计算、分布式系统、PaaS
 - 持续、迭代地分享工作成果，即使这些工作未完成

###识别需求

在我们开始分析数据之前，我们需要明确一下，我们的问题是什么？即，我们到底要干嘛，我们想要的内容是什么。

> 识别信息需求是确保数据分析过程有效性的首要条件，可以为收集数据、分析数据提供清晰的目标。

当我们想要提到我们的网站在不同的地区的速度时，我们就需要去探索我们的用户主要是在哪些地区。即，现在这是我们的需求。我们已经有了这样的一个明确的目标，下面要做起来就很轻松了。

###收集数据


那么现在新的问题来了，我们的数据要从哪里来？

对于大部分的网站来说，都会有访问日志。但是这些访问日志只能显示某个IP进入了某个页面，并不人详细地介绍这个用户在这个页面待了多久，做了什么事。这时候，这些数据就需要依赖于类似于Google Analytics这样的工具来统计网站的流量。还有类似于New Relic这样的工具来统计用户的一些行为。

在一些以科学研究为目的的数据收集中，我们可以从一些公开的数据中获取这些资料。

而在一些特殊的情况里，我们就需要通过爬虫来完成这样的工作。

###分析数据

现在，我们终于可以真正的去分析数据了——我的意思是，我们要开始写代码了。

####Hadoop 分析数据

> Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。它可以让用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。

Hadoop 的框架最核心的设计就是：HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，则 MapReduce 为海量的数据提供了计算。

HDF 是一个分布式文件系统（Hadoop Distributed File System）。它有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。同时，HDFS 放宽了（relax）POSIX 的要求，可以以流的形式访问（streaming access）文件系统中的数据。

MapReduce 是 Google 提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。概念“Map（映射）”和“Reduce（归纳）”，及他们的主要思想，都是从函数式编程语言借来的，还有从矢量编程语言借来的特性。

###展示数据

###改进

参考来源: **精益数据分析**。
